\documentclass[]{article}

\usepackage[]{siunitx}
\usepackage[]{fancyvrb}
\usepackage[]{booktabs}
\usepackage[]{flafter}
\usepackage[]{hyperref}
\DefineShortVerb{\|}

\setcounter{secnumdepth}{-2}

\title{CS 6210 Project 1 Report}
\author{Sam Britt}
\date{February 7, 2012}

\begin{document}
\maketitle

\section{Implementation}
\label{sec:implementation}

The overall structure of the design was to factor out all the
scheduler-specific code into modules that adhere to a generic
``scheduler'' interface, and hooks put in place throughout the
|kthread| and |uthread| modules to initiate scheduler-specific
functionality. In this way, the user can provide the scheduler she
wishes to use as an option to |gtthread_app_init()|, and the
scheduling policies can be loaded at runtime. Both the priority
scheduler provided with the package and the new completely fair
scheduler were designed to meet this interface. The interface includes
the functions:
\begin{itemize}
  \item |scheduler_init()| This function is called at the start of the
    application, and allows the scheduler to initialize any data
    structures it needs. The scheduler will have access to its data
    though a global variable.
  \item |kthread_init()| Called after the initialization of every
    |kthread|, to allow the scheduler to do any bookkeeping it may
    need to do for each |kthread|
  \item |uthread_init()| Called after the creation of every |uthread|.
    The scheduler must return an appropriate |kthread| for it to be
    scheduled on.
  \item |preempt_current_uthread()| Called after the timer interrupt
    to preempt the currently running |uthread|. It is here that the
    scheduler can insert the uthread into the appropriate ready queue,
    if needed.
  \item |pick_next_uthread()| Called to choose the next runnable
    |uthread|.
  \item |resume_uthread()| Called just before returning control back
    to the user's application. Its main function is to set a timer for
    the appropriate timeslice amount. The library will be woken at
    this point to schedule the next thread.
\end{itemize}
All these functions are set up through function pointers maintained in
a global |scheduler| data structure. The pointers are defined at
application startup, so there should be little or no performance lost
during the scheduling of |uthread|s.

For the completely fair scheduler, the data structures are as follows:
The scheduler maintains an array of |cfs_kthread| structures, one for
each virtual processor being used. Each |cfs_kthread| maintains a
pointer to the |kthread| structure used by the rest of the system, as
well as CFS-specific data. For example, each |cfs_kthread| has a
pointer to the red-black tree holding all the schedulable entities for
that processor, a pointer to the currently running |uthread|, the
current latency or ``epoch length'' for the system, the current load
on the system, and the current value of the minimum virtual runtime
(|vruntime|) of all the |uthread|s in its runqueue. Similarly, the
|cfs_uthread| structure maintains a pointer to the corresponding
|uthread| used by the rest of the system, in addition to the current
values of it's virtual runtime, its priority, and a pointer to its
node in the red-black tree.

At each point of the above interface, these data structures are
updated appropriately. For example, during |uthread_init()|, a node is
created with the current minimum |vruntime| and inserted into the
appropriate |cfs_kthread|'s red-black tree. In addition, the
|cfs_kthread|'s load is increased, and this potentially increases the
latency past a threshold. Similarly, when a |uthread| has completed
execution, it is removed from the red-black tree and the |kthread|
load is reduced.

When a |uthread| is preempted, the time it spent in execution is first
calculated. Its |vruntime| is then updated as
\begin{equation}
  \mathtt{vrutime} = \mathtt{cputime} \times \mathtt{priority}
\end{equation}
In this manner, higher priority (lower value) tasks have their
|vruntime| increase more slowly, keeping them more ``to the left'' in
the red-black tree. Lower priority tasks have their |vruntime|
increase quickly and will be further ``to the right,'' and thus will
wait longer to be scheduled. After the new |vruntime| is calculated,
it is inserted back into the red-black tree keyed on |vruntime| minus
the minimum |vruntime|. The subtraction handles possible overflow in
the |vruntime| value.

Picking the next |uthread| to schedule is a simple operation: simply
pick the node in the tree with the minimum |vruntime| value. The
|min_vruntime| value for the |cfs_kthread| is also updated with this
value.

Before the |uthread| resumes execution, a timer is set. The value of
this timeslice is determined at every context switch, and it depends
on the current load and |uthread| priority as
\begin{equation}
  \mathtt{timeslice} = \mathtt{latency} \times
  \frac{\mathtt{priority}}{\mathtt{load}}
\end{equation}
where |load| is simply the sum of the priorities of all the threads
waiting to be scheduled. In this way, high priority, interactive tasks
get smaller timeslices (and their |vruntime| stays small).

The compiled in default is a latency of \SI{20}{\milli\second}.
Virtual processors are chosen in a round-robin fashion; there is no
co-scheduling or grouping of |uthread|s. Times are to microsecond
resolution. The red-black tree implementation used can be found at
\url{http://www.mit.edu/~emin/source_code/red_black_tree/index.html}.
It was modified to support caching of the left-most node for quick
retrieval.

% section implementation (end)

\section{Results} 
\label{sec:results}

The scheduler was tested by having $128$ |uthread|s multiply square
matrices of different sizes in parallel. The results are shown in
Table~\ref{tab}.

Due to issues in the code (see below), I could not get the application
to run reliably on small matrices; that is, matrices with less than
about 64 elements per edge. Therefore, I started with matrices of size
128 and went to 512. All threads were running in parallel on their own
matrix; there were 32 threads per matrix size.

``CPU Time'' is the time the |uthread| spent actually on the
processor; that is, excluding the time waiting in the ready queue.
``Elapsed time'' is the amount of time between |uthread| creation and
the completion of its task; it includes all the overhead of scheduling
and waiting for other tasks.

The results are mostly as expected: the threads with larger matrices
showed higher execution times and variabilities. The increase in CPU
time is initially linear; for example; for example, doubling the
matrix size from 128 to 256 caused an 8-fold increase in CPU time
(which is linear because the larger matrix has 4 times as many
elements, and twice as many calculations to perform per element in the
``inner loop.''). The elapsed time, however, grows much more rapidly.

\begin{table}[h]
  \begin{center}
    \label{tab}
    \begin{minipage}{.8\linewidth}
      \caption{CPU time and total execution time when using $128$
        threads to multiply matrices of various sizes. The total
        application time was \SI{11.117}{\second}.}
      \begin{tabular}{c r r r r}
        \toprule
        Matrix Size
        & \multicolumn{2}{c}{CPU Time (\si{\micro\second})} 
        & \multicolumn{2}{c}{Elapsed Time (\si{\micro\second})} \\
        \cmidrule(r){2-3} \cmidrule(l){4-5}
        &   mean    &(std dev)  &      mean    & (std dev) \\
        \midrule
        128 & \num{4030}  &  (\num{1752})  &  \num{4028}   &  (\num{1752}) \\
        200 & \num{14459} &  (\num{3415})  &  \num{464505} &  (\num{409631}) \\
        256 & \num{32736} &  (\num{8049})  &  \num{936797} &  (\num{214329}) \\
        512 & \num{579843}&  (\num{59293}) &  \num{9215725}&  (\num{966977}) \\
        \bottomrule
      \end{tabular}
    \end{minipage}
  \end{center}
\end{table}

% section results (end)

\section{Implementation Issues} 
\label{sec:implemntation_issues}

Besides taking far more time than I had planned, the major issue is
some race condition that occurs most readily when the |uthread|s are
given short tasks to perform. Often, if the tasks can complete within
their initial timeslice, a segmentation fault or a deadlock will
occur. I have not been able to track this bug down. If the |uthread|s
are given longer tasks, so that they use their timeslices fully, the
issue seems to be rarer.


% section implemntation_issues (end)

\end{document}
% interim report:
\section{GT-Threads}
\label{sec:gtthreads_understanding}

The overall design of |gtthread| is a many-to-many model where one
kernel-level thread (|kthread|) is created for each CPU on the system
(and is scheduled by the kernel), but potentially many user-level
threads (|uthread|) are multiplexed and scheduled over them by
|gtthread|. It currently supports priority-based scheduling of threads
in a similar manner to the $O(1)$ scheduler previously in the Linux
kernel, in addition to a co-scheduler that attempts to ensure threads
of the same ``group'' execute concurrently. In the following sections,
I will give my understanding of the codebase by describing what
happens with the |gtthread| user initializes the application, creates
multiple |uthread|s, and then runs the application (and |uthread|s are
scheduled).

\subsection{Application Initialization}
\label{sub:application_initialization}

Before the user can create |uthread|s to parallelize his application,
he must first call |gtthread_app_init()|. This function does the
following:
\begin{enumerate}
  \item The process currently executing is recognized as the first
    |kthread|, and various data structures are initialized
    appropriately.
  \item A timer interrupt and co-scheduler interrupt handler are
    registered with the |SIGVTALRM| and |SIGUSR1| signals,
    respectively, and a virtual timer is started with a default
    timeslice.
  \item For each CPU in the system, a |kthread| is created (using
    |clone()|) and initialized, and it its CPU affinity is set to
    ensure each is scheduled by the kernel on a unique CPU.
  \item Each |kthread| is put in a loop of scheduling the next
    ``best'' |uthread| to run, until all |uthread|s have completed
    their work.
\end{enumerate}


% subsection application_initialization (end)

\subsection{Creating Threads}
\label{sub:creating_threads}

To parallelize his application, the user then calls |uthread_create()|
a number of times to create user-level threads. The user passes it a
function |start_routine| (with its argument) for the |uthread| to
execute, along with a group id for co-scheduling. When the new
|uthread| is created, the following occurs:

\begin{enumerate}
  \item Memory is allocated for the new |uthread|, and parameters
    are initialized to their defaults.
  \item The |uthread| is assigned to particular |kthread| (i.e., a
    virtual processor). This assignment occurs in a round-robin
    fashion.
  \item The |uthread| is provided its own stack in this way: an
    alternate stack for the |SIGUSR2| signal handler is established,
    and the signal is raised synchronously. When the handler returns,
    the alternate stack is disabled for signal handling, and becomes
    the normal stack for the |uthread|.
  \item The |uthread| is inserted into the run queue of its |kthread|,
    ready to be scheduled for execution.
\end{enumerate}

% subsection creating_threads (end)

\subsection{Application Runtime}
\label{sub:runtime}

During the running of the application, the |SIGVTALRM| signal is
raised at the end of every timeslice. It is my understanding that this
signal is only delivered to the original |kthread| (I do not think
that timers are shared across the |clone()| call, but I could be
wrong). When this signal is raised, the following occurs:

\begin{enumerate}
  \item The coscheduling group is announced (currently a no-op).
  \item For all |kthread|s other than the currently running thread, a
    signal is raised to trigger a context switch for co-scheduling
    (the so-called ``relay'' handler).
  \item The scheduling function |uthread_schedule()| is called, and is
    given a function to find the next ``best'' |uthread| in the
    current virtual processor's run queue. This function then
    performs the following:
    \begin{enumerate}
      \item The current |uthread| is marked as |RUNNABLE| and
        re-inserted into the run queue, or, if the |uthread| has
        finished, it is inserted into a zombie queue.
      \item The next |uthread| is selected using the provided
        function.
      \item If there are no more |uthread|s to run, the current
        |kthread| is marked as |DONE| and returns.
      \item Else, the new |uthread| is initialized if it hasn't been
        already, and the timer and relay signal handlers are
        re-installed.
      \item Using |setjmp| and |longjmp|, control flows to the
        |uthread|'s context, where execution of the user's function
        |start_routine| resumes.
      \item When the |uthread| finishes execution, it calls
        |uthread_schedule()| to force a new |uthread| to run.
    \end{enumerate}
\end{enumerate}

At program exit, the original |kthread| waits for all the other
|kthread|s to complete, and then returns.

% subsection runtime (end)

\subsection{Priority and Co-Scheduling Algorithms}
\label{sub:scheduling}

User-level thread's can be given one of $32$ priority levels, with
lower numbers meaning higher priority. The default priority is set at
$16$. I do not see an interface defined for changing a |uthread|'s
priority from the default, and the priorities do not seem to change
dynamically based on interactivity or otherwise. User-level threads
can also be put in one of $32$ groups for co-scheduling, but see
below.

Kernel-level threads maintain their |uthread|s in two queues, |active|
for currently scheduleable threads, and |expires|. The priority
scheduler, implemented in |sched_find_best_uthread()|, first
determines if there is a runnable |uthread| in the |active| queue.
If there is not, it switches the |active| and |expired| queues,
and again determines if there is a runnable |uthread| in the new
active queue. If one is still not found, it simply returns |NULL|.
Else if one is found, it determines the highest priority level that
has a runnable thread, and finds the |uthread| with the highest
priority group with that priority level. This thread is removed from
the active queue and is returned.

It seems that the co-scheduler is not fully implemented, or at least
not intended to be used. If used, all the |kthread|s would attempt to
schedule threads of the same group concurrently. However,
\begin{itemize}
  \item |ksched_announce_cosched_group()| is a no-op.
  \item |ksched_cosched()| and |sched_find_best_uthread_group()|
    depend on preprocessor definitions |CO_SCHED| and |COSCHED|,
    respectively, which are never defined. The routines fall back to
    using the standard priority thread scheduler.
  \item If |CO_SCHED| and |COSCHED| were defined, so that the
    co-scheduling routines were actually executed, it seems that the
    only group that would be checked would be group $0$.
\end{itemize}

% subsection scheduling (end)
% section gtthreads_understanding (end)

\section{Completely Fair Scheduler}
\label{sec:cfs_understanding}

The advantage of the $O(1)$ scheduler is that the next task to be
scheduled can be determined in constant time, regardless of number of
tasks running on the system. However, once a task gets moved from the
active to the expired queue, it could potentially wait a very long
time before it gets scheduled again, even if it has high priority. The
Completely Fair Scheduler (CFS) attempts to correct this while
sacrificing the $O(1)$ behavior.

The CFS attempts to \emph{fairly} distribute (virtual) runtime equally
among all its tasks classified as ``normal'' (as opposed to real-time
or batch tags). The algorithm is essentially greedy: tasks are sorted
by virtual runtimes, the task with least amount of virtual runtime is
scheduled to execute next, and its virtual runtime is reduced by an
appropriate amount. The scheduling is greatly simplified: there are no
multiple priority queues, no active/expired queues, and no heuristics
to determine interactivity. Priorities are taken into account by
adjusting the rate at which virtual runtimes increase---high priority
tasks have their virtual runtimes increase slowly so they will more
rapidly move up the queue and will be scheduled again. Timeslices
determined by the task's remaining virtual runtime and the system
load, and change each time the task is scheduled.

The queue of tasks is actually implemented as a red-black tree keyed
on virtual runtime. A red-black tree is a self-balancing tree with
$O(\log n)$ operations. However, since the tree is always dequeued from
the left-most node (lowest virtual runtime), a pointer can be
maintained to make the dequeue an $O(1)$ operation. However, when the
preempted task is put back into the tree, it will have a new virtual
runtime and the insert is an $O(\log n)$ operation.

% section cfs_understanding (end)

\section{Design Sketch}
\label{sec:design_sketch}

It seems that most of the changes that need to be made to |gtthread|
to support are in the file |gt_pq.c|, where the priority queue data
structures and scheduling algorithms (e.g.,
|sched_find_best_uthread()|) are found. The red-black tree data
structure, as well as routines and structures specific to CFS (e.g.,
calculating virtual runtime) will need to be implemented.

Since each |uthread| will now have timeslices that change each time it
is scheduled, all the |kthreads| will have to register virtual timers.
Somehow the |kthread| will have to interface with the CFS data
structures to determine what to set the timer to. Also, when the user
executes the |gt_yield()|, the |SIGVTALRM| signal will have to be
raised to force the proper calculation of remaining virtual runtime
and the scheduling of a new thread.

I would like maintain the implementation of the priority scheduler,
and allow the user to choose a scheduler implementation when calling
|gtthread_app_init()|, providing some default of course. I believe the
priority scheduler and the CFS could implement the same ``scheduler''
interface, providing generic functions such as |pick_runnable_task()|
and |insert_task_into_queue()|. In this way, signal handlers receiving
timer interrupts can simply pass along this information to whatever
scheduler the system has been configured with, without coupling too
tightly (e.g., the use of |ksched_priority()| as a signal handler).
Care has to be taken here with respect to performance, as the
scheduler cannot be a bottleneck. Still, with the proper design, all
the necessary function pointers could be initialized in
|gtthread_app_init()| with minimal cost while the application is
running.

% section design_sketch (end)

